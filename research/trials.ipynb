{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone Github repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\admin\\\\genaibatch\\\\Source_CodeAnalysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers==2.2.2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (4.39.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sentence-transformers==2.2.2) (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from tqdm->sentence-transformers==2.2.2) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from torchvision->sentence-transformers==2.2.2) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\Users\\\\admin\\\\genaibatch\\\\Source_CodeAnalysis\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/entbappy/End-to-end-ML-Project-Implementation\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src/mlProject',\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import DataIngestionConfig\\nfrom pathlib import Path\\n\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config = config\\n\\n    \\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    \\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\n\\nclass DataTransformation:\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n\\n    def train_test_spliting(self):\\n        data = pd.read_csv(self.config.data_path)\\n\\n        # Split the data into training and test sets. (0.75, 0.25) split.\\n        train, test = train_test_split(data)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\\n\\n        logger.info(\"Splited data into training and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)\\n        ', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom mlProject import logger\\nimport pandas as pd\\nfrom mlProject.entity.config_entity import DataValidationConfig\\n                                    \\n\\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            \\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.utils.common import save_json\\nfrom urllib.parse import urlparse\\nimport numpy as np\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom pathlib import Path\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n\\n    \\n    def eval_metrics(self,actual, pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))\\n        mae = mean_absolute_error(actual, pred)\\n        r2 = r2_score(actual, pred)\\n        return rmse, mae, r2\\n    \\n\\n\\n    def save_results(self):\\n\\n        test_data = pd.read_csv(self.config.test_data_path)\\n        model = joblib.load(self.config.model_path)\\n\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        test_y = test_data[[self.config.target_column]]\\n        \\n        predicted_qualities = model.predict(test_x)\\n\\n        (rmse, mae, r2) = self.eval_metrics(test_y, predicted_qualities)\\n        \\n        # Saving metrics as local\\n        scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n        save_json(path=Path(self.config.metric_file_name), data=scores)\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import pandas as pd\\nimport os\\nfrom mlProject import logger\\nfrom sklearn.linear_model import ElasticNet\\nimport joblib\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\n\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    \\n    def train(self):\\n        train_data = pd.read_csv(self.config.train_data_path)\\n        test_data = pd.read_csv(self.config.test_data_path)\\n\\n\\n        train_x = train_data.drop([self.config.target_column], axis=1)\\n        test_x = test_data.drop([self.config.target_column], axis=1)\\n        train_y = train_data[[self.config.target_column]]\\n        test_y = test_data[[self.config.target_column]]\\n\\n\\n        lr = ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig,\\n                                            DataValidationConfig,\\n                                            DataTransformationConfig,\\n                                            ModelTrainerConfig,\\n                                            ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(\\n        self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n        \\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config = self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config = DataIngestionConfig(\\n            root_dir=config.root_dir,\\n            source_URL=config.source_URL,\\n            local_data_file=config.local_data_file,\\n            unzip_dir=config.unzip_dir \\n        )\\n\\n        return data_ingestion_config\\n    \\n\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema=schema,\\n        )\\n\\n        return data_validation_config\\n    \\n\\n    \\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config = self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config = DataTransformationConfig(\\n            root_dir=config.root_dir,\\n            data_path=config.data_path,\\n        )\\n\\n        return data_transformation_config\\n    \\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config = self.config.model_trainer\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config = ModelTrainerConfig(\\n            root_dir=config.root_dir,\\n            train_data_path = config.train_data_path,\\n            test_data_path = config.test_data_path,\\n            model_name = config.model_name,\\n            alpha = params.alpha,\\n            l1_ratio = params.l1_ratio,\\n            target_column = schema.name\\n            \\n        )\\n\\n        return model_trainer_config\\n    \\n\\n\\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\\n        config = self.config.model_evaluation\\n        params = self.params.ElasticNet\\n        schema =  self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config = ModelEvaluationConfig(\\n            root_dir=config.root_dir,\\n            test_data_path=config.test_data_path,\\n            model_path = config.model_path,\\n            all_params=params,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name\\n           \\n        )\\n\\n        return model_evaluation_config', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path\\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path\\n\\n\\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    alpha: float\\n    l1_ratio: float\\n    target_column: str\\n\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    all_params: dict\\n    metric_file_name: Path\\n    target_column: str', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"import joblib \\nimport numpy as np\\nimport pandas as pd\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model = joblib.load(Path('artifacts/model_trainer/model.joblib'))\\n\\n    \\n    def predict(self, data):\\n        prediction = self.model.predict(data)\\n\\n        return prediction\", metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion stage\"\\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config=data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Data Validation stage\"\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation = DataValiadtion(config=data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject import logger\\nfrom pathlib import Path\\n\\n\\nSTAGE_NAME = \"Data Transformation stage\"\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\\n                status = f.read().split(\" \")[-1]\\n\\n            if status == \"True\":\\n                config = ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config=data_transformation_config)\\n                data_transformation.train_test_spliting()\\n\\n            else:\\n                raise Exception(\"You data schema is not valid\")\\n\\n        except Exception as e:\\n            print(e)\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\nfrom mlProject import logger\\n\\n\\n\\nSTAGE_NAME = \"Model Trainer stage\"\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_trainer_config = config.get_model_trainer_config()\\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\\n        model_trainer_config.train()\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\nfrom mlProject import logger\\n\\n\\nSTAGE_NAME = \"Model evaluation stage\"\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config = ConfigurationManager()\\n        model_evaluation_config = config.get_model_evaluation_config()\\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\\n        model_evaluation_config.save_results()\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HuggingFace\"] = \"***************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\genaibatch\\Source_CodeAnalysis\\llmapp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447726368904114, 0.03102320246398449, 0.006734968163073063, 0.026108931750059128, -0.03936202451586723, -0.16030243039131165, 0.06692398339509964, -0.006441442761570215, -0.04745052754878998, 0.014758856035768986, 0.07087531685829163, 0.05552757903933525, 0.019193340092897415, -0.026251325383782387, -0.010109522379934788, -0.02694045566022396, 0.02230742760002613, -0.022226622328162193, -0.14969255030155182, -0.0174929890781641, 0.007676256354898214, 0.05435231328010559, 0.0032544289715588093, 0.03172588720917702, -0.08462140709161758, -0.029405998066067696, 0.05159555748105049, 0.04812406003475189, -0.003314817091450095, -0.05827919766306877, 0.04196927323937416, 0.022210652008652687, 0.128188818693161, -0.02233898639678955, -0.011656259186565876, 0.06292835623025894, -0.03287637606263161, -0.0912260189652443, -0.031175369396805763, 0.05269956216216087, 0.04703478515148163, -0.0842030867934227, -0.030056225135922432, -0.02074485272169113, 0.009517816826701164, -0.0037217906210571527, 0.007343289442360401, 0.039324358105659485, 0.0932740718126297, -0.003788599045947194, -0.052742112427949905, -0.0580582395195961, -0.006864347029477358, 0.005283215083181858, 0.082893006503582, 0.01936275325715542, 0.006284528877586126, -0.01033075712621212, 0.009032411500811577, -0.03768370300531387, -0.045206114649772644, 0.024016348645091057, -0.006944166962057352, 0.013491620309650898, 0.10005497932434082, -0.07168389856815338, -0.021695096045732498, 0.0316183902323246, -0.05163465440273285, -0.08224773406982422, -0.06569329649209976, -0.009895346127450466, 0.005816427059471607, 0.07355450838804245, -0.0340503565967083, 0.024886082857847214, 0.0144881010055542, 0.026457352563738823, 0.009656732901930809, 0.030217235907912254, 0.05280391871929169, -0.07535986602306366, 0.009897169657051563, 0.029836829751729965, 0.01755562238395214, 0.02309195138514042, 0.0019338327692821622, 0.0014002168318256736, -0.047175999730825424, -0.011194328777492046, -0.11420145630836487, -0.019811933860182762, 0.04026627168059349, 0.002192968502640724, -0.07979220151901245, -0.025382278487086296, 0.09448299556970596, -0.028981050476431847, -0.14500246942043304, 0.230977401137352, 0.027731185778975487, 0.03211148828268051, 0.031065011397004128, 0.04283284768462181, 0.06423778086900711, 0.03216312825679779, -0.00487676914781332, 0.0556994192302227, -0.03753235936164856, -0.02150552347302437, -0.02834271267056465, -0.028846899047493935, 0.03835304081439972, -0.01746862381696701, 0.05248529464006424, -0.07487603276968002, -0.03125975653529167, 0.02184160239994526, -0.039895690977573395, -0.008587086573243141, 0.026956548914313316, -0.04849550500512123, 0.011469871737062931, 0.029618220403790474, -0.020572232082486153, 0.013103869743645191, 0.028833476826548576, -3.1941979827962646e-33, 0.06478209793567657, -0.01813017763197422, 0.05178992077708244, 0.12198276817798615, 0.028780192136764526, 0.008722028695046902, -0.07052112370729446, -0.016907326877117157, 0.04073970392346382, 0.04211613908410072, 0.025447174906730652, 0.03574622794985771, -0.04914475977420807, 0.002129051834344864, -0.015546551905572414, 0.050730545073747635, -0.0481853224337101, 0.03588061407208443, -0.0040670400485396385, 0.10172475129365921, -0.05597006529569626, -0.010681059211492538, 0.011235787533223629, 0.09068651497364044, 0.0042344448156654835, 0.03513865917921066, -0.009702871553599834, -0.09386514872312546, 0.0928555503487587, 0.00800495594739914, -0.007705414667725563, -0.0520867183804512, -0.01258799061179161, 0.0032669196370989084, 0.006013536360114813, 0.0075815897434949875, 0.010517144575715065, -0.08634555339813232, -0.06987878680229187, -0.0025339112617075443, -0.09097662568092346, 0.046887341886758804, 0.052076540887355804, 0.00719384104013443, 0.010903670452535152, -0.005229501985013485, 0.013937338255345821, 0.02196834608912468, 0.03420862555503845, 0.06022470071911812, 0.00011661567259579897, 0.014731964096426964, -0.07008923590183258, 0.028499050065875053, -0.02760169468820095, 0.010768401436507702, 0.03483098745346069, -0.022487876936793327, 0.009769012220203876, 0.07722778618335724, 0.021588314324617386, 0.11495619267225266, -0.06800112873315811, 0.023761000484228134, -0.015983929857611656, -0.017826952040195465, 0.06439489126205444, 0.03202567622065544, 0.05027022585272789, -0.005913750268518925, -0.033707987517118454, 0.01784028485417366, 0.016573335975408554, 0.06329656392335892, 0.034677162766456604, 0.04647345468401909, 0.09790614247322083, -0.006635509897023439, 0.02520706132054329, -0.07798831909894943, 0.016926420852541924, -0.0009457948035560548, 0.02247188799083233, -0.038253214210271835, 0.09570479393005371, -0.005350829102098942, 0.010469106957316399, -0.11524055153131485, -0.013262538239359856, -0.010709444060921669, -0.0831172838807106, 0.07327354699373245, 0.049392230808734894, -0.008994358591735363, -0.0958455502986908, 3.3661485617505796e-33, 0.12493185698986053, 0.019349711015820503, -0.05822574719786644, -0.0359882153570652, -0.05074671283364296, -0.045662377029657364, -0.08260340243577957, 0.14819476008415222, -0.08842123299837112, 0.06027442589402199, 0.05103016272187233, 0.010303194634616375, 0.14121422171592712, 0.03081381693482399, 0.06103312224149704, -0.052851270884275436, 0.1366489678621292, 0.009189935401082039, -0.01732517033815384, -0.012848643586039543, -0.007995317690074444, -0.05098005011677742, -0.05235062539577484, 0.007593025919049978, -0.015166323632001877, 0.01696031726896763, 0.02127058431506157, 0.020558087155222893, -0.120028056204319, 0.014461832121014595, 0.026759905740618706, 0.025330694392323494, -0.042754631489515305, 0.006768444553017616, -0.014458554796874523, 0.04526202380657196, -0.09147648513317108, -0.019439062103629112, -0.01783350110054016, -0.054910119622945786, -0.05264106020331383, -0.010459048673510551, -0.05201602354645729, 0.02089199796319008, -0.07997032254934311, -0.012111307121813297, -0.05773143842816353, 0.023178240284323692, -0.008031771518290043, -0.025989297777414322, -0.07995670288801193, -0.020728878676891327, 0.048817675560712814, -0.02038913406431675, -0.049176592379808426, 0.014159692451357841, -0.06362206488847733, -0.00780738377943635, 0.01643151417374611, -0.025682469829916954, 0.013381186872720718, 0.02624877728521824, 0.00997837446630001, 0.06322884559631348, 0.002672168891876936, -0.00658276304602623, 0.01663189008831978, 0.03236643970012665, 0.03794243931770325, -0.036376070231199265, -0.006910913623869419, 0.00015965377679094672, -0.0016334975371137261, -0.027278177440166473, -0.028038091957569122, 0.04968142881989479, -0.02886722981929779, -0.0024180591572076082, 0.014774901792407036, 0.009764560498297215, 0.005797590594738722, 0.01348614227026701, 0.005567940883338451, 0.0372270867228508, 0.0072324867360293865, 0.040156230330467224, 0.08150330185890198, 0.0719916820526123, -0.013056143186986446, -0.04288202524185181, -0.011011239141225815, 0.004897777456790209, -0.009229704737663269, 0.035191502422094345, -0.051035039126873016, -1.571437557856825e-08, -0.08862439543008804, 0.02390933595597744, -0.016238732263445854, 0.031700484454631805, 0.027284253388643265, 0.05246879532933235, -0.047070953994989395, -0.05884748697280884, -0.06320816278457642, 0.040888529270887375, 0.04982798546552658, 0.10655166953802109, -0.07450228929519653, -0.01249542087316513, 0.01837068609893322, 0.03947411850094795, -0.02479790896177292, 0.014516258612275124, -0.03706921264529228, 0.020015694200992584, -4.8627563955960795e-05, 0.009866558015346527, 0.024838784709572792, -0.05245818942785263, 0.02931414730846882, -0.08719190955162048, -0.01449974998831749, 0.02601906657218933, -0.018746323883533478, -0.07620514184236526, 0.03504325821995735, 0.10363951325416565, -0.028050484135746956, 0.012718172743916512, -0.07632548362016678, -0.018652360886335373, 0.0249767005443573, 0.08144529163837433, 0.06875883787870407, -0.06405656039714813, -0.0838937759399414, 0.0613623671233654, -0.033545512706041336, -0.10615331679582596, -0.04008052870631218, 0.03253021091222763, 0.07662482559680939, -0.07301618903875351, 0.0003375500382389873, -0.040871672332286835, -0.0757884755730629, 0.027527684345841408, 0.0746254250407219, 0.01771729812026024, 0.0912184864282608, 0.11022016406059265, 0.0005698063177987933, 0.05146333575248718, -0.014551352709531784, 0.03323204815387726, 0.023792242631316185, -0.02288980968296528, 0.0389375314116478, 0.030206819996237755]\n"
     ]
    }
   ],
   "source": [
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (0.5.14)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Downloading langsmith-0.1.38-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-core) (1.10.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Downloading orjson-3.10.0-cp38-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "     ------------------------ --------------- 30.7/50.7 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 50.7/50.7 kB 861.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from pydantic<3,>=1->langchain-core) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.9 MB 3.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.7/1.9 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.0/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.9 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.2/1.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.3/1.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.9 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.7/1.9 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
      "   ---------------------------------------- 0.0/279.2 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/279.2 kB 1.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/279.2 kB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 194.6/279.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 256.0/279.2 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 279.2/279.2 kB 1.2 MB/s eta 0:00:00\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.1.38-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/86.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.9/86.9 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading orjson-3.10.0-cp38-none-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/139.0 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/139.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 81.9/139.0 kB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ---- 122.9/139.0 kB 901.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.0/139.0 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: packaging, orjson, jsonpointer, langsmith, jsonpatch, langchain-core, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.92\n",
      "    Uninstalling langsmith-0.0.92:\n",
      "      Successfully uninstalled langsmith-0.0.92\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-community-0.0.31 langchain-core-0.1.38 langsmith-0.1.38 orjson-3.10.0 packaging-23.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.0.249 requires langsmith<0.1.0,>=0.0.11, but you have langsmith 0.1.38 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (0.0.249)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (0.0.31)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (0.1.38)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (0.1.38)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\genaibatch\\source_codeanalysis\\llmapp\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
      "   ---------------------------------------- 0.0/812.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/812.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 61.4/812.8 kB 825.8 kB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 204.8/812.8 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 358.4/812.8 kB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 624.6/812.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 812.8/812.8 kB 3.2 MB/s eta 0:00:00\n",
      "Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-text-splitters, langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.249\n",
      "    Uninstalling langchain-0.0.249:\n",
      "      Successfully uninstalled langchain-0.0.249\n",
      "Successfully installed langchain-0.1.14 langchain-text-splitters-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",huggingfacehub_api_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "from mlProject.config.configuration import ConfigurationManager\n",
      "from mlProject.components.model_evaluation import ModelEvaluation\n",
      "from mlProject import logger\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Model evaluation stage\"\n",
      "\n",
      "class ModelEvaluationTrainingPipeline:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "    def main(self):\n",
      "        config = ConfigurationManager()\n",
      "        model_evaluation_config = config.get_model_evaluation_config()\n",
      "        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
      "        model_evaluation_config.save_results()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    try:\n",
      "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
      "        obj = ModelEvaluationTrainingPipeline()\n",
      "        obj.main()\n",
      "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        raise e\n",
      "\n",
      "import os\n",
      "from box.exceptions import BoxValueError\n",
      "import yaml\n",
      "from mlProject import logger\n",
      "import json\n",
      "import joblib\n",
      "from ensure import ensure_annotations\n",
      "from box import ConfigBox\n",
      "from pathlib import Path\n",
      "from typing import Any\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def read_yaml(path_to_yaml: Path) -> ConfigBox:\n",
      "    \"\"\"reads yaml file and returns\n",
      "\n",
      "    Args:\n",
      "        path_to_yaml (str): path like input\n",
      "\n",
      "    Raises:\n",
      "        ValueError: if yaml file is empty\n",
      "        e: empty file\n",
      "\n",
      "    Returns:\n",
      "        ConfigBox: ConfigBox type\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(path_to_yaml) as yaml_file:\n",
      "            content = yaml.safe_load(yaml_file)\n",
      "            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n",
      "            return ConfigBox(content)\n",
      "    except BoxValueError:\n",
      "        raise ValueError(\"yaml file is empty\")\n",
      "    except Exception as e:\n",
      "        raise e\n",
      "    \n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def create_directories(path_to_directories: list, verbose=True):\n",
      "    \"\"\"create list of directories\n",
      "\n",
      "    Args:\n",
      "        path_to_directories (list): list of path of directories\n",
      "        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n",
      "    \"\"\"\n",
      "    for path in path_to_directories:\n",
      "        os.makedirs(path, exist_ok=True)\n",
      "        if verbose:\n",
      "            logger.info(f\"created directory at: {path}\")\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "def save_json(path: Path, data: dict):\n",
      "    \"\"\"save json data\n",
      "\n",
      "    Args:\n",
      "        path (Path): path to json file\n",
      "        data (dict): data to be saved in json file\n",
      "    \"\"\"\n",
      "    with open(path, \"w\") as f:\n",
      "        json.dump(data, f, indent=4)\n",
      "\n",
      "    logger.info(f\"json file saved at: {path}\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@ensure_annotations\n",
      "\n",
      "from mlProject.constants import *\n",
      "from mlProject.utils.common import read_yaml, create_directories\n",
      "from mlProject.entity.config_entity import (DataIngestionConfig,\n",
      "                                            DataValidationConfig,\n",
      "                                            DataTransformationConfig,\n",
      "                                            ModelTrainerConfig,\n",
      "                                            ModelEvaluationConfig)\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: what is DataIngestion class?\n",
      "AI: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "from mlProject.config.configuration import ConfigurationManager\n",
      "from mlProject.components.data_transformation import DataTransformation\n",
      "from mlProject import logger\n",
      "from pathlib import Path\n",
      "\n",
      "\n",
      "STAGE_NAME = \"Data Transformation stage\"\n",
      "\n",
      "class DataTransformationTrainingPipeline:\n",
      "    def __init__(self):\n",
      "        pass\n",
      "\n",
      "\n",
      "    def main(self):\n",
      "        try:\n",
      "            with open(Path(\"artifacts/data_validation/status.txt\"), \"r\") as f:\n",
      "                status = f.read().split(\" \")[-1]\n",
      "\n",
      "            if status == \"True\":\n",
      "                config = ConfigurationManager()\n",
      "                data_transformation_config = config.get_data_transformation_config()\n",
      "                data_transformation = DataTransformation(config=data_transformation_config)\n",
      "                data_transformation.train_test_spliting()\n",
      "\n",
      "            else:\n",
      "                raise Exception(\"You data schema is not valid\")\n",
      "\n",
      "        except Exception as e:\n",
      "            print(e)\n",
      "\n",
      "class ConfigurationManager:\n",
      "    def __init__(\n",
      "        self,\n",
      "        config_filepath = CONFIG_FILE_PATH,\n",
      "        params_filepath = PARAMS_FILE_PATH,\n",
      "        schema_filepath = SCHEMA_FILE_PATH):\n",
      "\n",
      "        self.config = read_yaml(config_filepath)\n",
      "        self.params = read_yaml(params_filepath)\n",
      "        self.schema = read_yaml(schema_filepath)\n",
      "\n",
      "        create_directories([self.config.artifacts_root])\n",
      "        \n",
      "\n",
      "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
      "        config = self.config.data_ingestion\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_ingestion_config = DataIngestionConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            source_URL=config.source_URL,\n",
      "            local_data_file=config.local_data_file,\n",
      "            unzip_dir=config.unzip_dir \n",
      "        )\n",
      "\n",
      "        return data_ingestion_config\n",
      "    \n",
      "\n",
      "    \n",
      "    def get_data_validation_config(self) -> DataValidationConfig:\n",
      "        config = self.config.data_validation\n",
      "        schema = self.schema.COLUMNS\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_validation_config = DataValidationConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            STATUS_FILE=config.STATUS_FILE,\n",
      "            unzip_data_dir = config.unzip_data_dir,\n",
      "            all_schema=schema,\n",
      "        )\n",
      "\n",
      "        return data_validation_config\n",
      "    \n",
      "\n",
      "    \n",
      "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
      "        config = self.config.data_transformation\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "        data_transformation_config = DataTransformationConfig(\n",
      "            root_dir=config.root_dir,\n",
      "            data_path=config.data_path,\n",
      "        )\n",
      "\n",
      "        return data_transformation_config\n",
      "    \n",
      "\n",
      "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
      "        config = self.config.model_trainer\n",
      "        params = self.params.ElasticNet\n",
      "        schema =  self.schema.TARGET_COLUMN\n",
      "\n",
      "        create_directories([config.root_dir])\n",
      "\n",
      "from mlProject.constants import *\n",
      "from mlProject.utils.common import read_yaml, create_directories\n",
      "from mlProject.entity.config_entity import (DataIngestionConfig,\n",
      "                                            DataValidationConfig,\n",
      "                                            DataTransformationConfig,\n",
      "                                            ModelTrainerConfig,\n",
      "                                            ModelEvaluationConfig)\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is DataIngestion class?\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is DataIngestion class?\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is DataIngestion class?\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is\n",
      "Helpful Answer: The DataIngestion class is not defined in the provided code. It is likely defined in a different file or module.\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is the DataTransformationTrainingPipeline class?\n",
      "Standalone question: What is the DataTransformationTrainingPipeline class?\n",
      "\n",
      "Chat History:\n",
      "\n",
      "system: \n",
      "Follow Up Input: what is the Data\n",
      "\n",
      "New summary:\n",
      "The human asks what the DataIngestion class is. The AI does not know the answer because the DataIngestion class is not defined in the provided code. The human then asks what the DataTransformationTrainingPipeline class is.\n",
      "Follow Up Input: what is DataIngestion class?\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "New lines of conversation:\n",
      "Human: What is the DataIngestion class?\n",
      "AI: The DataIngestion class is not defined in the provided code. It is likely defined in a different file or module.\n",
      "\n",
      "New summary:\n",
      "The human asks what the DataIngestion class is. The AI does not know the answer because the DataIngestion class is not defined in the provided code. The human then\n",
      "Helpful Answer: The DataIngestion class is not defined in the provided code. It is likely defined in a different file or module.\n",
      "Standalone question: What is the DataIngestion class?\n",
      "\n",
      "New lines of conversation:\n",
      "Human: What is the DataIngestion class?\n",
      "AI: The DataIngestion class is not defined in the provided code. It is likely defined in a different file or module.\n",
      "\n",
      "New summary:\n",
      "The human asks what the DataIn\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"*************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\genaibatch\\Source_CodeAnalysis\\llmapp\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data_openai')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\genaibatch\\Source_CodeAnalysis\\llmapp\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\genaibatch\\Source_CodeAnalysis\\llmapp\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `DataIngestion` class is to handle the downloading and extraction of files based on the configuration provided. In the context given, the `DataIngestion` class is instantiated with the `data_ingestion_config` obtained from the `ConfigurationManager`. It then uses this configuration to download a file from a specified URL and extract a zip file to a designated directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
